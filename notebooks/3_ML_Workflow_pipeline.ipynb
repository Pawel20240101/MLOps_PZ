{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad02aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Instalacja i import niezbędnych bibliotek\n",
    "\n",
    "# Import bibliotek\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_curve, auc, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from mlflow.models.signature import infer_signature\n",
    "import optuna\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Tworzenie potrzebnych katalogów\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "os.makedirs(\"../reports\", exist_ok=True)\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "# Ustawienie stylu dla wykresów\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Konfiguracja MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")  # Zmień na adres swojego serwera MLflow\n",
    "experiment_name = \"Heart_Disease_Classification_test_6\"  # Nazwa eksperymentu\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment ID: {mlflow.get_experiment_by_name(experiment_name).experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef853a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Wczytywanie danych przygotowanych wcześniej\n",
    "\n",
    "# Wczytywanie danych treningowych i testowych\n",
    "train_data = pd.read_csv(\"../data/processed/heart_train.csv\")\n",
    "test_data = pd.read_csv(\"../data/processed/heart_test.csv\")\n",
    "\n",
    "# Podział na cechy i etykietę (target)\n",
    "X_train = train_data.drop(\"HeartDisease\", axis=1)\n",
    "y_train = train_data[\"HeartDisease\"].map({\"No\": 0, \"Yes\": 1})\n",
    "\n",
    "X_test = test_data.drop(\"HeartDisease\", axis=1)\n",
    "y_test = test_data[\"HeartDisease\"].map({\"No\": 0, \"Yes\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5caf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Przygotowanie danych – identyfikacja kolumn\n",
    "\n",
    "# Identyfikacja kolumn kategorycznych i numerycznych\n",
    "categorical_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numerical_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "print(\"Kolumny kategoryczne:\", categorical_cols)\n",
    "print(\"Kolumny numeryczne:\", numerical_cols)\n",
    "print(f\"Wymiary zbioru treningowego: {X_train.shape}\")\n",
    "print(f\"Wymiary zbioru testowego: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21df7286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: Przygotowanie pipeline do przetwarzania danych\n",
    "\n",
    "# Definiowanie preprocessingu dla kolumn numerycznych\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Definiowanie preprocessingu dla kolumn kategorycznych\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Połączenie transformerów w jeden preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Funkcja do tworzenia pipeline'u modelu\n",
    "def create_model_pipeline(classifier):\n",
    "    \"\"\"Tworzy pipeline dla modelu, który zawiera preprocessor i klasyfikator.\"\"\"\n",
    "    return Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74223287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Trenowanie modeli i logowanie wyników do MLflow\n",
    "\n",
    "# Lista klasyfikatorów do przetestowania\n",
    "classifiers = {\n",
    "    \"LinearSVC\": LinearSVC(random_state=42, max_iter=20000),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', n_jobs=-1),\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1),\n",
    "    #\"RandomForestClassifier\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42, n_jobs=-1),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(),\n",
    "    #\"LGBMClassifier\": LGBMClassifier(random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0, random_state=42),\n",
    "    \"RidgeClassifier\": RidgeClassifier(),\n",
    "    #\"NaiveBayes\": GaussianNB(),\n",
    "    \"LDA\": LinearDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "# Słownik do przechowywania wyników\n",
    "results = {}\n",
    "\n",
    "# Funkcja do logowania modelu i metryk do MLflow\n",
    "def log_model_to_mlflow(model, model_name, pipeline, X_test, y_test):\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    if hasattr(pipeline, \"predict_proba\"):\n",
    "        y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(pipeline, \"decision_function\"):\n",
    "        y_prob = pipeline.decision_function(X_test)\n",
    "    else:\n",
    "        y_prob = None\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name, nested=True):\n",
    "        for param_name, param_value in model.get_params().items():\n",
    "            mlflow.log_param(param_name, param_value)\n",
    "\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'Confusion Matrix - {model_name}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        cm_path = f\"../results/confusion_matrix_{model_name}.png\"\n",
    "        plt.savefig(cm_path)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(cm_path)\n",
    "\n",
    "        # ROC Curve\n",
    "        if y_prob is not None:\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f'ROC Curve - {model_name}')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            roc_path = f\"../results/roc_curve_{model_name}.png\"\n",
    "            plt.savefig(roc_path)\n",
    "            plt.close()\n",
    "            mlflow.log_artifact(roc_path)\n",
    "            mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        else:\n",
    "            roc_auc = None\n",
    "\n",
    "        signature = infer_signature(X_test, y_pred)\n",
    "\n",
    "        # Bezpieczne logowanie modelu + artefaktu lokalnie\n",
    "        local_model_path = f\"../models/{model_name}_model.pkl\"\n",
    "        joblib.dump(pipeline, local_model_path)\n",
    "\n",
    "        # Log lokalnego pliku jako artefakt (nie przez API model registry)\n",
    "        mlflow.log_artifact(local_model_path)\n",
    "\n",
    "        # Próbuj logować do MLflow registry, ale jeśli serwer zamknie połączenie – pomiń\n",
    "        try:\n",
    "            mlflow.sklearn.log_model(\n",
    "                pipeline,\n",
    "                f\"{model_name}_model\",\n",
    "                input_example=X_test.iloc[:1],\n",
    "                signature=signature\n",
    "            )\n",
    "            mlflow.register_model(\n",
    "                model_uri=f\"runs:/{mlflow.active_run().info.run_id}/{model_name}_model\",\n",
    "                name=model_name\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Nie udało się zarejestrować modelu {model_name} w MLflow Model Registry: {e}\")\n",
    "\n",
    "        # Classification report\n",
    "        clf_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        clf_df = pd.DataFrame(clf_report).transpose()\n",
    "        clf_path = f\"../reports/classification_report_{model_name}.csv\"\n",
    "        clf_df.to_csv(clf_path)\n",
    "        mlflow.log_artifact(clf_path)\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"roc_auc\": roc_auc\n",
    "        }\n",
    "\n",
    "# Trenowanie i logowanie modeli\n",
    "for name, classifier in tqdm(classifiers.items(), desc=\"Trenowanie modeli\"):\n",
    "    print(f\"\\nTrenowanie modelu: {name}\")\n",
    "    pipeline = create_model_pipeline(classifier)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    result = log_model_to_mlflow(classifier, name, pipeline, X_test, y_test)\n",
    "    results[name] = result\n",
    "    print(f\"Model: {name}, Accuracy: {result['accuracy']:.4f}, F1 Score: {result['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d03dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: Porównanie wyników modeli\n",
    "\n",
    "# Porównanie wyników wszystkich modeli\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nPodsumowanie wyników wszystkich modeli:\")\n",
    "print(results_df)\n",
    "\n",
    "# Zapis wyników do pliku CSV i logowanie jako artefakt do MLflow\n",
    "results_csv_path = \"../reports/model_results_summary.csv\"\n",
    "results_df.to_csv(results_csv_path)\n",
    "with mlflow.start_run(run_name=\"Model_Comparison_Summary\", nested=True):\n",
    "    mlflow.log_artifact(results_csv_path)\n",
    "\n",
    "# Wizualizacja porównania metryk\n",
    "plt.figure(figsize=(12, 8))\n",
    "results_df[['accuracy', 'precision', 'recall', 'f1_score']].plot(kind='bar')\n",
    "plt.title('Porównanie wyników modeli')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Wartość metryki')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../results/model_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7: Tuning hiperparametrów najlepszego modelu (Optuna)\n",
    "\n",
    "# Wybór najlepszego modelu na podstawie accuracy\n",
    "best_model_name = results_df['accuracy'].idxmax()\n",
    "print(f\"\\nNajlepszy model: {best_model_name} z accuracy: {results_df.loc[best_model_name, 'accuracy']:.4f}\")\n",
    "\n",
    "# Funkcja celu Optuny\n",
    "def optuna_objective(trial):\n",
    "    if best_model_name == \"RandomForestClassifier\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 5, 30),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 4)\n",
    "        }\n",
    "        model = RandomForestClassifier(random_state=42, **params)\n",
    "\n",
    "    elif best_model_name == \"XGBoost\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "        }\n",
    "        model = XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42, **params)\n",
    "\n",
    "    elif best_model_name == \"LGBMClassifier\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
    "        }\n",
    "        model = LGBMClassifier(random_state=42, **params)\n",
    "\n",
    "    elif best_model_name == \"CatBoost\":\n",
    "        params = {\n",
    "            \"iterations\": trial.suggest_int(\"iterations\", 100, 500),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
    "        }\n",
    "        model = CatBoostClassifier(verbose=0, random_state=42, **params)\n",
    "\n",
    "    elif best_model_name == \"GradientBoosting\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "        }\n",
    "        model = GradientBoostingClassifier(random_state=42, **params)\n",
    "    else:\n",
    "        raise ValueError(\"Model nieobsługiwany przez Optuna.\")\n",
    "\n",
    "    # Pipeline i predykcje\n",
    "    pipeline = create_model_pipeline(model)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    preds = pipeline.predict(X_test)\n",
    "\n",
    "    # Zwrotka dla Optuny — f1 jako metryka\n",
    "    return f1_score(y_test, preds, average='weighted')\n",
    "\n",
    "# Uruchomienie Optuny\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(optuna_objective, n_trials=30)\n",
    "\n",
    "# Najlepsze parametry i model\n",
    "best_params = study.best_trial.params\n",
    "print(f\"\\nNajlepsze parametry znalezione przez Optunę: {best_params}\")\n",
    "\n",
    "# Odtworzenie najlepszego modelu\n",
    "if best_model_name == \"RandomForestClassifier\":\n",
    "    final_model = RandomForestClassifier(random_state=42, **best_params)\n",
    "elif best_model_name == \"XGBoost\":\n",
    "    final_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, **best_params)\n",
    "elif best_model_name == \"LGBMClassifier\":\n",
    "    final_model = LGBMClassifier(random_state=42, **best_params)\n",
    "elif best_model_name == \"CatBoost\":\n",
    "    final_model = CatBoostClassifier(verbose=0, random_state=42, **best_params)\n",
    "elif best_model_name == \"GradientBoosting\":\n",
    "    final_model = GradientBoostingClassifier()\n",
    "elif best_model_name == \"KNN\":\n",
    "    final_model = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', n_jobs=-1)\n",
    "elif best_model_name == \"LinearSVC\":\n",
    "    final_model = LinearSVC(random_state=42, max_iter=20000)\n",
    "elif best_model_name == \"LogisticRegression\":\n",
    "    final_model = LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1)\n",
    "elif best_model_name == \"RidgeClassifier\":\n",
    "    final_model = RidgeClassifier()\n",
    "elif best_model_name == \"NaiveBayes\":\n",
    "    final_model = GaussianNB()\n",
    "elif best_model_name == \"LDA\":\n",
    "    final_model = LinearDiscriminantAnalysis()\n",
    "else:\n",
    "    raise ValueError(\"Brak implementacji final_model dla wybranego modelu.\")\n",
    "\n",
    "# Trenowanie finalnego pipeline\n",
    "final_pipeline = create_model_pipeline(final_model)\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "# Metryki końcowe\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_precision = precision_score(y_test, y_pred, average='weighted')\n",
    "test_recall = recall_score(y_test, y_pred, average='weighted')\n",
    "test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nMetryki modelu po tuningu Optuna:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "# Wykres SHAP tylko dla modeli wspieranych\n",
    "if best_model_name in [\"XGBoost\", \"LGBMClassifier\", \"CatBoost\", \"RandomForestClassifier\"]:\n",
    "    X_transformed = final_pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "    explainer = shap.Explainer(final_pipeline.named_steps['classifier'], X_transformed)\n",
    "    shap_values = explainer(X_transformed, check_additivity=False)\n",
    "\n",
    "    plt.figure()\n",
    "    shap.summary_plot(\n",
    "        shap_values,\n",
    "        features=X_transformed,\n",
    "        feature_names=final_pipeline.named_steps['preprocessor'].get_feature_names_out(),\n",
    "        show=False\n",
    "    )\n",
    "    shap_path = f\"../results/shap_summary_{best_model_name}.png\"\n",
    "    plt.savefig(shap_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Logowanie wykresu SHAP jako artefakt\n",
    "    with mlflow.start_run(run_name=f\"SHAP_{best_model_name}\", nested=True):\n",
    "        mlflow.log_artifact(shap_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a64f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8: Zapis najlepszego modelu do pliku\n",
    "\n",
    "# Zapisanie finalnego pipeline z najlepszymi parametrami do pliku\n",
    "model_path = f\"../models/{best_model_name}_best_model.pkl\"\n",
    "joblib.dump(final_pipeline, model_path)\n",
    "\n",
    "print(f\"\\nNajlepszy model zapisany do: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9: Predict pipeline – generowanie predykcji\n",
    "\n",
    "def predict_pipeline(data_to_predict, model_path):\n",
    "    \"\"\"\n",
    "    Przetwarzanie nowych danych i generowanie predykcji.\n",
    "    \n",
    "    Args:\n",
    "        data_to_predict: DataFrame z nowymi danymi\n",
    "        model_path: Ścieżka do zapisanego modelu\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame z predykcjami i prawdopodobieństwami (jeśli dostępne)\n",
    "    \"\"\"\n",
    "    # Wczytanie modelu\n",
    "    loaded_model = joblib.load(model_path)\n",
    "\n",
    "    # Generowanie predykcji\n",
    "    predictions = loaded_model.predict(data_to_predict)\n",
    "\n",
    "    # Prawdopodobieństwa (jeśli dostępne)\n",
    "    if hasattr(loaded_model, \"predict_proba\"):\n",
    "        probabilities = loaded_model.predict_proba(data_to_predict)[:, 1]\n",
    "        return pd.DataFrame({\n",
    "            'prediction': predictions,\n",
    "            'probability': probabilities\n",
    "        })\n",
    "    else:\n",
    "        return pd.DataFrame({\n",
    "            'prediction': predictions\n",
    "        })\n",
    "\n",
    "# Przykład użycia na 5 próbkach\n",
    "print(\"\\nDemonstracja działania predict pipeline:\")\n",
    "sample_data = X_test.iloc[:5]\n",
    "predictions = predict_pipeline(sample_data, model_path)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0095d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10: Podsumowanie projektu\n",
    "\n",
    "# Podsumowanie\n",
    "print(\"\\nPodsumowanie projektu:\")\n",
    "print(f\"1. Najlepszy model: {best_model_name}\")\n",
    "print(f\"2. Najlepsze parametry (Optuna): {best_params}\")\n",
    "print(f\"3. Accuracy najlepszego modelu: {test_accuracy:.4f}\")\n",
    "print(f\"4. F1 Score najlepszego modelu: {test_f1:.4f}\")\n",
    "print(f\"5. Precision najlepszego modelu: {test_precision:.4f}\")\n",
    "print(f\"6. Recall najlepszego modelu: {test_recall:.4f}\")\n",
    "\n",
    "print(\"\\nWszystkie eksperymenty i wyniki są dostępne w MLflow pod adresem:\")\n",
    "print(f\"MLflow UI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Experiment ID: {mlflow.get_experiment_by_name(experiment_name).experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1784623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11: Zakończenie\n",
    "print(\"\\nProjekt zakończony sukcesem!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
